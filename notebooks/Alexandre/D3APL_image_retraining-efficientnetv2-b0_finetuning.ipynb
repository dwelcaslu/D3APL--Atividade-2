{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScitaPqhKtuW"
   },
   "source": [
    "##### Copyright 2021 The TensorFlow Hub Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYM61xrTsP5d"
   },
   "source": [
    "# Retraining an Image Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1otmJgmbahf"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Image classification models have millions of parameters. Training them from\n",
    "scratch requires a lot of labeled training data and a lot of computing power. Transfer learning is a technique that shortcuts much of this by taking a piece of a model that has already been trained on a related task and reusing it in a new model.\n",
    "\n",
    "This Colab demonstrates how to build a Keras model for classifying five species of flowers by using a pre-trained TF2 SavedModel from TensorFlow Hub for image feature extraction, trained on the much larger and more general ImageNet dataset. Optionally, the feature extractor can be trained (\"fine-tuned\") alongside the newly added classifier.\n",
    "\n",
    "### Looking for a tool instead?\n",
    "\n",
    "This is a TensorFlow coding tutorial. If you want a tool that just builds the TensorFlow or TFLite model for, take a look at the [make_image_classifier](https://github.com/tensorflow/hub/tree/master/tensorflow_hub/tools/make_image_classifier) command-line tool that gets [installed](https://www.tensorflow.org/hub/installation) by the PIP package `tensorflow-hub[make_image_classifier]`, or at [this](https://colab.sandbox.google.com/github/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/demo/image_classification.ipynb) TFLite colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bL54LWCHt5q5"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dlauq-4FWGZM",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.9.1\n",
      "Hub version: 0.12.0\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Hub version:\", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmaHHH7Pvmth"
   },
   "source": [
    "## Select the TF2 SavedModel module to use\n",
    "\n",
    "For starters, use [https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4](https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4). The same URL can be used in code to identify the SavedModel and in your browser to show its documentation. (Note that models in TF1 Hub format won't work here.)\n",
    "\n",
    "You can find more TF2 models that generate image feature vectors [here](https://tfhub.dev/s?module-type=image-feature-vector&tf-version=tf2).\n",
    "\n",
    "There are multiple possible models to try. All you need to do is select a different one on the cell below and follow up with the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FlsEcKVeuCnf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: efficientnetv2-b0 : https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\n",
      "Input size (224, 224)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"efficientnetv2-b0\" # @param ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\n",
    "\n",
    "model_handle_map = {\n",
    "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2\",\n",
    "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2\",\n",
    "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2\",\n",
    "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\",\n",
    "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1\",\n",
    "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1\",\n",
    "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\",\n",
    "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1\",\n",
    "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1\",\n",
    "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1\",\n",
    "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\",\n",
    "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/1\",\n",
    "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/feature-vector/4\",\n",
    "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4\",\n",
    "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4\",\n",
    "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4\",\n",
    "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4\",\n",
    "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\",\n",
    "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4\",\n",
    "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4\",\n",
    "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4\",\n",
    "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\",\n",
    "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\n",
    "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n",
    "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4\",\n",
    "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\",\n",
    "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\n",
    "}\n",
    "\n",
    "\n",
    "model_image_size_map = {\n",
    "  \"efficientnetv2-s\": 384,\n",
    "  \"efficientnetv2-m\": 480,\n",
    "  \"efficientnetv2-l\": 480,\n",
    "  \"efficientnetv2-b0\": 224,\n",
    "  \"efficientnetv2-b1\": 240,\n",
    "  \"efficientnetv2-b2\": 260,\n",
    "  \"efficientnetv2-b3\": 300,\n",
    "  \"efficientnetv2-s-21k\": 384,\n",
    "  \"efficientnetv2-m-21k\": 480,\n",
    "  \"efficientnetv2-l-21k\": 480,\n",
    "  \"efficientnetv2-xl-21k\": 512,\n",
    "  \"efficientnetv2-b0-21k\": 224,\n",
    "  \"efficientnetv2-b1-21k\": 240,\n",
    "  \"efficientnetv2-b2-21k\": 260,\n",
    "  \"efficientnetv2-b3-21k\": 300,\n",
    "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
    "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
    "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
    "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
    "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
    "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
    "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
    "  \"efficientnetv2-b3-21k-ft1k\": 300, \n",
    "  \"efficientnet_b0\": 224,\n",
    "  \"efficientnet_b1\": 240,\n",
    "  \"efficientnet_b2\": 260,\n",
    "  \"efficientnet_b3\": 300,\n",
    "  \"efficientnet_b4\": 380,\n",
    "  \"efficientnet_b5\": 456,\n",
    "  \"efficientnet_b6\": 528,\n",
    "  \"efficientnet_b7\": 600,\n",
    "  \"inception_v3\": 299,\n",
    "  \"inception_resnet_v2\": 299,\n",
    "  \"nasnet_large\": 331,\n",
    "  \"pnasnet_large\": 331,\n",
    "}\n",
    "\n",
    "model_handle = model_handle_map.get(model_name)\n",
    "pixels = model_image_size_map.get(model_name, 224)\n",
    "\n",
    "print(f\"Selected model: {model_name} : {model_handle}\")\n",
    "\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "print(f\"Input size {IMAGE_SIZE}\")\n",
    "\n",
    "BATCH_SIZE = 16#@param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTY8qzyYv3vl"
   },
   "source": [
    "## Set up the Flowers dataset\n",
    "\n",
    "Inputs are suitably resized for the selected module. Dataset augmentation (i.e., random distortions of an image each time it is read) improves training, esp. when fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WBtFK1hO8KsO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"../../data/pubfig83/imgs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "umB5tswsfTEQ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13838 files belonging to 83 classes.\n",
      "Using 11071 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-16 22:19:11.749397: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3\n",
      "Found 13838 files belonging to 83 classes.\n",
      "Using 2767 files for validation.\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(subset):\n",
    "  return tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      data_dir,\n",
    "      validation_split=.20,\n",
    "      subset=subset,\n",
    "      label_mode=\"categorical\",\n",
    "      # Seed needs to provided when using validation_split and shuffle = True.\n",
    "      # A fixed seed is used so that the validation set is stable across runs.\n",
    "      seed=123,\n",
    "      image_size=IMAGE_SIZE,\n",
    "      batch_size=1)\n",
    "\n",
    "train_ds = build_dataset(\"training\")\n",
    "class_names = tuple(train_ds.class_names)\n",
    "train_size = train_ds.cardinality().numpy()\n",
    "train_ds = train_ds.unbatch().batch(BATCH_SIZE)\n",
    "train_ds = train_ds.repeat()\n",
    "\n",
    "normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
    "preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
    "do_data_augmentation = True #@param {type:\"boolean\"}\n",
    "if do_data_augmentation:\n",
    "  preprocessing_model.add(\n",
    "      tf.keras.layers.RandomRotation(40))\n",
    "  preprocessing_model.add(\n",
    "      tf.keras.layers.RandomTranslation(0, 0.2))\n",
    "  preprocessing_model.add(\n",
    "      tf.keras.layers.RandomTranslation(0.2, 0))\n",
    "  # Like the old tf.keras.preprocessing.image.ImageDataGenerator(),\n",
    "  # image sizes are fixed when reading, and then a random zoom is applied.\n",
    "  # If all training inputs are larger than image_size, one could also use\n",
    "  # RandomCrop with a batch size of 1 and rebatch later.\n",
    "  preprocessing_model.add(\n",
    "      tf.keras.layers.RandomZoom(0.2, 0.2))\n",
    "  preprocessing_model.add(\n",
    "      tf.keras.layers.RandomFlip(mode=\"horizontal\"))\n",
    "train_ds = train_ds.map(lambda images, labels:\n",
    "                        (preprocessing_model(images), labels))\n",
    "\n",
    "val_ds = build_dataset(\"validation\")\n",
    "valid_size = val_ds.cardinality().numpy()\n",
    "val_ds = val_ds.unbatch().batch(BATCH_SIZE)\n",
    "val_ds = val_ds.map(lambda images, labels:\n",
    "                    (normalization_layer(images), labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS_gVStowW3G"
   },
   "source": [
    "## Defining the model\n",
    "\n",
    "All it takes is to put a linear classifier on top of the `feature_extractor_layer` with the Hub module.\n",
    "\n",
    "For speed, we start out with a non-trainable `feature_extractor_layer`, but you can also enable fine-tuning for greater accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RaJW3XrPyFiF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "do_fine_tuning = True #@param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "50FYNIb1dmJH",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 1280)              5919312   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 83)                106323    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,025,635\n",
      "Trainable params: 5,965,027\n",
      "Non-trainable params: 60,608\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Building model with\", model_handle)\n",
    "model = tf.keras.Sequential([\n",
    "    # Explicitly define the input shape so the model can be properly\n",
    "    # loaded by the TFLiteConverter\n",
    "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "    hub.KerasLayer(model_handle, trainable=do_fine_tuning),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(len(class_names),\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "])\n",
    "model.build((None,)+IMAGE_SIZE+(3,))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2e5WupIw2N2"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9f3yBUvkd_VJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9), \n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_YKX2Qnfg6x",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-16 22:19:27.022118: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 25690112 exceeds 10% of free system memory.\n",
      "2022-06-16 22:19:27.044332: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 25690112 exceeds 10% of free system memory.\n",
      "2022-06-16 22:19:27.054664: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 25690112 exceeds 10% of free system memory.\n",
      "2022-06-16 22:19:27.059908: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 25690112 exceeds 10% of free system memory.\n",
      "2022-06-16 22:19:27.105773: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 25690112 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691/691 [==============================] - 905s 1s/step - loss: 4.1174 - accuracy: 0.0818 - val_loss: 3.5753 - val_accuracy: 0.1781\n",
      "Epoch 2/100\n",
      "691/691 [==============================] - 776s 1s/step - loss: 3.2764 - accuracy: 0.2544 - val_loss: 2.7435 - val_accuracy: 0.3921\n",
      "Epoch 3/100\n",
      "691/691 [==============================] - 774s 1s/step - loss: 2.6418 - accuracy: 0.4313 - val_loss: 2.2437 - val_accuracy: 0.5632\n",
      "Epoch 4/100\n",
      "691/691 [==============================] - 774s 1s/step - loss: 2.2227 - accuracy: 0.5689 - val_loss: 1.9429 - val_accuracy: 0.6599\n",
      "Epoch 5/100\n",
      "691/691 [==============================] - 771s 1s/step - loss: 1.9657 - accuracy: 0.6521 - val_loss: 1.7761 - val_accuracy: 0.7093\n",
      "Epoch 6/100\n",
      "691/691 [==============================] - 770s 1s/step - loss: 1.7993 - accuracy: 0.7085 - val_loss: 1.6363 - val_accuracy: 0.7515\n",
      "Epoch 7/100\n",
      "691/691 [==============================] - 770s 1s/step - loss: 1.6588 - accuracy: 0.7616 - val_loss: 1.5796 - val_accuracy: 0.7667\n",
      "Epoch 8/100\n",
      "691/691 [==============================] - 770s 1s/step - loss: 1.5438 - accuracy: 0.7956 - val_loss: 1.5032 - val_accuracy: 0.8005\n",
      "Epoch 9/100\n",
      "691/691 [==============================] - 770s 1s/step - loss: 1.4625 - accuracy: 0.8271 - val_loss: 1.4549 - val_accuracy: 0.8143\n",
      "Epoch 10/100\n",
      "691/691 [==============================] - 770s 1s/step - loss: 1.3943 - accuracy: 0.8493 - val_loss: 1.4027 - val_accuracy: 0.8372\n",
      "Epoch 11/100\n",
      "691/691 [==============================] - 770s 1s/step - loss: 1.3311 - accuracy: 0.8721 - val_loss: 1.3673 - val_accuracy: 0.8470\n",
      "Epoch 12/100\n",
      "691/691 [==============================] - 770s 1s/step - loss: 1.2744 - accuracy: 0.8922 - val_loss: 1.3240 - val_accuracy: 0.8623\n",
      "Epoch 13/100\n",
      "691/691 [==============================] - 770s 1s/step - loss: 1.2300 - accuracy: 0.9066 - val_loss: 1.3179 - val_accuracy: 0.8616\n",
      "Epoch 14/100\n",
      "691/691 [==============================] - 770s 1s/step - loss: 1.2076 - accuracy: 0.9139 - val_loss: 1.2772 - val_accuracy: 0.8775\n",
      "Epoch 15/100\n",
      "691/691 [==============================] - 770s 1s/step - loss: 1.1754 - accuracy: 0.9252 - val_loss: 1.2587 - val_accuracy: 0.8772\n",
      "Epoch 16/100\n",
      "691/691 [==============================] - 770s 1s/step - loss: 1.1360 - accuracy: 0.9387 - val_loss: 1.2364 - val_accuracy: 0.8808\n",
      "Epoch 17/100\n",
      "691/691 [==============================] - 770s 1s/step - loss: 1.1126 - accuracy: 0.9460 - val_loss: 1.2427 - val_accuracy: 0.8808\n",
      "Epoch 18/100\n",
      "691/691 [==============================] - 770s 1s/step - loss: 1.0929 - accuracy: 0.9498 - val_loss: 1.2403 - val_accuracy: 0.8841\n",
      "Epoch 19/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 1.0817 - accuracy: 0.9544 - val_loss: 1.2161 - val_accuracy: 0.8946\n",
      "Epoch 20/100\n",
      "691/691 [==============================] - 767s 1s/step - loss: 1.0576 - accuracy: 0.9614 - val_loss: 1.2195 - val_accuracy: 0.8888\n",
      "Epoch 21/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 1.0459 - accuracy: 0.9622 - val_loss: 1.1832 - val_accuracy: 0.9004\n",
      "Epoch 22/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 1.0321 - accuracy: 0.9673 - val_loss: 1.1884 - val_accuracy: 0.8979\n",
      "Epoch 23/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 1.0222 - accuracy: 0.9687 - val_loss: 1.1793 - val_accuracy: 0.9023\n",
      "Epoch 24/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 1.0079 - accuracy: 0.9736 - val_loss: 1.1620 - val_accuracy: 0.9070\n",
      "Epoch 25/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 1.0022 - accuracy: 0.9750 - val_loss: 1.1647 - val_accuracy: 0.9019\n",
      "Epoch 26/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 0.9885 - accuracy: 0.9768 - val_loss: 1.1605 - val_accuracy: 0.9102\n",
      "Epoch 27/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 0.9848 - accuracy: 0.9774 - val_loss: 1.1774 - val_accuracy: 0.9004\n",
      "Epoch 28/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 0.9688 - accuracy: 0.9805 - val_loss: 1.1614 - val_accuracy: 0.9059\n",
      "Epoch 29/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 0.9645 - accuracy: 0.9828 - val_loss: 1.1517 - val_accuracy: 0.9066\n",
      "Epoch 30/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 0.9560 - accuracy: 0.9862 - val_loss: 1.1317 - val_accuracy: 0.9124\n",
      "Epoch 31/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 0.9558 - accuracy: 0.9834 - val_loss: 1.1270 - val_accuracy: 0.9153\n",
      "Epoch 32/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 0.9450 - accuracy: 0.9864 - val_loss: 1.1335 - val_accuracy: 0.9175\n",
      "Epoch 33/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 0.9399 - accuracy: 0.9880 - val_loss: 1.1203 - val_accuracy: 0.9150\n",
      "Epoch 34/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 0.9377 - accuracy: 0.9870 - val_loss: 1.1082 - val_accuracy: 0.9215\n",
      "Epoch 35/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 0.9274 - accuracy: 0.9886 - val_loss: 1.1224 - val_accuracy: 0.9175\n",
      "Epoch 36/100\n",
      "691/691 [==============================] - 769s 1s/step - loss: 0.9276 - accuracy: 0.9889 - val_loss: 1.1029 - val_accuracy: 0.9190\n",
      "Epoch 37/100\n",
      "691/691 [==============================] - 770s 1s/step - loss: 0.9266 - accuracy: 0.9881 - val_loss: 1.1143 - val_accuracy: 0.9132\n",
      "Epoch 38/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 0.9155 - accuracy: 0.9912 - val_loss: 1.1148 - val_accuracy: 0.9153\n",
      "Epoch 39/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 0.9148 - accuracy: 0.9896 - val_loss: 1.0918 - val_accuracy: 0.9190\n",
      "Epoch 40/100\n",
      "691/691 [==============================] - 769s 1s/step - loss: 0.9081 - accuracy: 0.9910 - val_loss: 1.1031 - val_accuracy: 0.9222\n",
      "Epoch 41/100\n",
      "691/691 [==============================] - 768s 1s/step - loss: 0.9048 - accuracy: 0.9925 - val_loss: 1.0983 - val_accuracy: 0.9241\n",
      "Epoch 42/100\n",
      "691/691 [==============================] - 775s 1s/step - loss: 0.9050 - accuracy: 0.9912 - val_loss: 1.0878 - val_accuracy: 0.9262\n",
      "Epoch 43/100\n",
      "369/691 [===============>..............] - ETA: 5:58 - loss: 0.9020 - accuracy: 0.9912"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = train_size // BATCH_SIZE\n",
    "validation_steps = valid_size // BATCH_SIZE\n",
    "hist = model.fit(\n",
    "    train_ds,\n",
    "    epochs=100, steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=validation_steps).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYOw0fTO1W4x",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss (training and validation)\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,4])\n",
    "plt.plot(hist[\"loss\"])\n",
    "plt.plot(hist[\"val_loss\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.ylabel(\"Accuracy (training and validation)\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,1])\n",
    "plt.plot(hist[\"accuracy\"])\n",
    "plt.plot(hist[\"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZ8DKKgeKv4-"
   },
   "source": [
    "Try out the model on an image from the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oi1iCNB9K1Ai",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(val_ds))\n",
    "image = x[0, :, :, :]\n",
    "true_index = np.argmax(y[0])\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Expand the validation image to (1, 224, 224, 3) before predicting the label\n",
    "prediction_scores = model.predict(np.expand_dims(image, axis=0))\n",
    "predicted_index = np.argmax(prediction_scores)\n",
    "print(\"True label: \" + class_names[true_index])\n",
    "print(\"Predicted label: \" + class_names[predicted_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCsAsQM1IRvA"
   },
   "source": [
    "Finally, the trained model can be saved for deployment to TF Serving or TFLite (on mobile) as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGvTi69oIc2d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "saved_model_path = f\"/tmp/saved_flowers_model_{model_name}\"\n",
    "tf.saved_model.save(model, saved_model_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ScitaPqhKtuW"
   ],
   "name": "TF Hub for TF2: Retraining an image classifier",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
